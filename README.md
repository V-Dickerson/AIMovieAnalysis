# AIMovieAnalysis
This project uses pandas, ChatGPT, and a dataset of movies and their ratings in order to test the capabilities of generative text models in classification for the purpose of expanded data analysis. 

# Background
Generative text models have been growing in popularity in recent months. ChatGPT, recently released to the public by OpenAI, is capable of synthesizing unique text that previously had to be generated by humans. With the rise of this technology, there have been many questions as to the future of its application. Some are more scared, asking "will ChatGPT replace my job?", while others see an opportunity.

# The idea
The idea tested in this paper is that the classification skills of generative text models have broader applications in data science as they can enable users to turn insignificant data sets into larger, more robust, and most importantly, reliable data sets. 

# The implementation
In order to test this, the first step was to find a data set. For this application, a dataset called "tmdb_movies_dataset" was used. You can find that [here](https://www.kaggle.com/datasets/afreentyagi/10000-tmdb-movies-dataset). This dataset provides the names, ratings, and descriptions of 10,000 movies. For the purposes of this application, the descriptions weren't used insofar as the goal was to generate something similar with the model.

The goal was to generate key terms for every movie on the list using a generative model, cleaning up the data to account for errors within GPT's processing, and then finally to find the statistical correlations between ratings and terms for a broad range of movies.

Taking the dataset from its provided form and converting it into something that fits our applications was simple. Here's a snippet of the code used to do so.
```py
def initialize_movie_data():
    with open(read_file, newline='\n', encoding="utf-8") as csv_file:
        movie_reader = csv.reader(csv_file, delimiter=',')
        id = 0
        for line in movie_reader:
            try:
                movie = {
                    'name': line[2],
                    'GPT_description': "",
                    'popularity': line[4],
                    'vote_average': line[6],
                    'vote_count': line[7]

                }
                movie_frame.loc[id] = movie
                #print(movie)
                id += 1
            except Exception as e:
                print(e)
                break
        movie_frame.to_csv("movie_by_terms.csv", sep=',', index=False, encoding='utf-8')
        print(movie_frame)
```

Once the data was in a more usable form, then began the implementation of the AI. The first challenge within that was finding a prompt that would reliably bring clean output. Depending on the model, some text would be more usable than others. Using OpenAI's Ada, it was incapable of following simple directions and describing the movie. However, jumping to ChatGPT 3.5, the model currently active on OpenAI's public site at the time of this writing, it was capable of listening to directions much better.

The final prompt chosen was `Act as a machine that lists the generic themes of provided movies. Provide a list of 15 themes for" + movie + " to help movie-goers know if they would enjoy the movie. The answer must be in the form of a semicolon separated list without any introduction.`. There's a few things of note here. First, the model is told to roleplay as a listing machine. While this may sound insignificant, asking these models to roleplay as some kind of a character can help them to provide more reliable inputs. Furthermore, we ask ChatGPT to provide a list of 15 themes to help movie-goers. Note that it won't return 15 themes -- it's restricted in how much it can output. However, the aim here is to get as many as it will provide and hopefully more creative answers. And finally, we tell the AI exactly how to deliniate the results, as to provide consistent and usable data.

Here's a snippet of the code used to get responses from ChatGPT. 
```py
# Takes movie name and returns the key GPT terms
def run_GPT(movie):
    print(f"RUNNING GPT CHECK ON {movie}")
    content = "Act as a machine that lists the generic themes of provided movies. Provide a list of 15 themes for" + movie + " to help movie-goers know if they would enjoy the movie. The answer must be in the form of a semicolon separated list without any introduction."
    message=[
        {"role": "user", "content": content},
    ]
    chat = openai.ChatCompletion.create(
        model="gpt-3.5-turbo", messages=message,
        max_tokens=20, temperature=1.5
    )
    return chat.choices[0].message["content"]
```

Now, a solution was necessary to loop through our dataset and update it with each response. In doing so, there were a few key problems to begin with. The first was that quite frequently, ChatGPT would be overloaded while calling requests, and it would return something like this --
```
That model is currently overloaded with other requests. You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID <request_id> in your message.)
```
This posed a problem because without proper handling, it could cause our program to crash, and it would leave gaps within the data. However, this problem was easily solved. First, including error handling that passed through each error request, but more importantly, through how the data was set. 
The dataset was read into a pandas DataFrame, and while looping through the data, it would check for NaN values within the descriptions. Those would exist on any that the model hadn't addressed yet. Then, if there was one of these values, it would call ChatGPT to provide the value for the dataset. Every 25 movies it would save the data, as to keep the temporary data held by the script to a minimum.

This allowed errors to carefully be skipped, and then for the dataset to be fixed upon later iterations, because the requests that returned errors would still be evaluated as NaN by pandas. Here's the code that was used to do that --
```py
def update_movie_details():
    data_set = pd.read_csv('movie_by_terms.csv')
    index = 0
    for i in range(400):
        for j in range(25):
            try:
                if pd.isna(data_set['GPT_description'][i * 25 + j]):
                    print(i * 25 + j)
                    data_set["GPT_description"][i * 25 + j] = run_GPT(data_set["name"][i * 25 + j])
            except Exception as e:
                print(e)
                pass
        data_set.to_csv('movie_by_terms.csv', sep=',', index=False, encoding='utf-8')
```
Note that multiprocessing was considered and would be viable for further and larger implementations, however it was decided to be outside of the scope of this project because the dataset was only 10,000 items, and would take a relatively small amount of time to iterate through so as to make multiprocessing not worth the effort.
